---
title: 水晶球里的缤纷万象（二）
excerpt: "IT老兵的QCon 2018见闻 —— Day 2"
categories: [tech]
tags: [conference]
toc: true
toc_sticky: true
---

## Day 2

京城从昨夜就开始下雨，淅淅沥沥的一直绵延到了今天早上，丝毫不没有停的意思，气温也降了不少。本以为下雨和降温会对今天QCon会场的人气有所影响，殊不知一到会场就发现好多会议室都是人满为患。

### RandonDB，NewSQL大胆实践者

今天参加的第一场演讲只赶上了一个尾巴，不过会场里倒是偶遇了两位来自链家的同行，于是攀谈了几句。第二场演讲我选择了位于同一会场，来自青云的“新一代分布式数据库架构设计”，演讲者是TokuDB的内核维护者之一BohuTANG，内容主要涉及的是青云正在研发并即将开源的一款分布式关系数据库RandonDB的架构设计思路。个人认为，开发分布式关系数据库是一项艰苦卓绝而又风险颇大的工作，不过以BohuTANG的技术背景，这样的选择倒也不觉得奇怪。好在也不是从零打造，RandonDB是构建于传统关系型数据库MySQL之上的，因此依赖了很多MySQL的功能。

当下，业内对NoSQL数据库的使用应该说已经是相当普及了，但NoSQL在满足高效存取海量数据的同时，对事务的ACID却支持不够。于是近年来NewSQL的概念应运而生，其主旨就是要从传统关系型数据库入手，探寻支持高并发与海量存储的同时，满足高可靠性事务的可能性。RandonDB就是NewSQL思想的一种实现。它底层以MySQL作为存储节点，上层构筑分布式SQL节点，将一条SQL查询解析成分布式执行计划，由多个执行器并行执行。之所以选择MySQL而非一般的KV作为存储，是因为其所具备的诸多优势，如：应用广泛，稳定可靠，以及多索引写原子保证等。

不难看出，RandonDB作为一款分布式关系型数据库，充分利用了MySQL的固有特性，并且提供了相当完备的功能支持。比如：基于Raft协议进行故障选主；利用semi sync确保数据零丢失；订阅bin log实现实时流式同步；借助MySQL原生的XA支持，实现SI级别的分布式事务隔离；数据分区在扩容情况下的热表动态漂移，并通过checksum校验保证数据迁移的质量；还有backup/restore，云部署，以及资源监控。

### RocketMQ，国人开源的成功典范

接下来的一场是来自阿里的有关RocketMQ的介绍。由于QCon官方网站尚未提供有关这一演讲的幻灯，而我在撰写整理此文时已是数天以后了，因而对这次演讲的细节已然印象不深，所以只能大略说一下。RocketMQ和Kafka同属消息中间件范畴，相比于Kafka擅长日志处理，其在消息可靠性方面表现更佳，因而适合对事务要求较高的场景，如：交易，充值。

RocketMQ由阿里中间件团队研发，与2012至2013年间对外开源，并于2016年捐赠给Apache软件基金会，次年从Incubator项目毕业。由于RocketMQ在研发期间经历过双十一的洗礼，性能与稳定性表现颇佳，能与国外顶尖开源软件项目比肩。此外，项目的原创团队联合业内厂商共同发起并起草了消息领域的标准化规范OpenMessaging，其在业界的影响力可见一斑。不可否认，RocketMQ是国人运作开源项目的成功典范。

### 百度网盘与Pinterest广告，两个大数据案例

下午两场，再次进入瞌睡模式。第一场现在已经记不得听的是哪个演讲了。第二场则是来自百度网盘的架构演进分享。依稀记得大致内容讲的是，百度网盘在面对海量信息存储，十亿日增文件数的挑战时，所采取的各种应对措施。其中印象较深的，是对物理机房进行抽象，并屏蔽IDC的差异，通过对资源和用户的描述，利用合理的分配算法，实现对网络资源的统一管理。并且，百度网盘针对系统的优化，已从“弹性可扩展”进一步深入到了“提高系统利用率以降低成本”的阶段，能够实现在高并发情况下的“削峰填谷”。

接下来的演讲，我选择的是来自Pinterest工程师带来的，“用Kafka Streams搭建实时的广告消费系统”。Pinterest的广告系统拥有非常实时的消费数据，因此很适合用Kafka Streams这样的技术来处理。有别于像Spark Streaming，Storm，Flink这样的流处理技术，Kafka Streams是一个非常轻量的客户端类库，可以内嵌于应用程序中，对存储于Kafka中的数据进行处理和分析，并输出至Kafka或外部系统。该讲座主要讨论的，是在Pinterest内部，团队是如何利用Kafka Streams来解决实时广告系统中的一些典型问题的，比如：避免超投（overdelivery），双流合并（stream-stream join），消息乱序，以及经典的“Exactly-once”问题。其中也提到Kafka Streams使用过程中的一些典型概念，比如：基于RocksDB的本地存储，Stream的join和aggregation操作，基于window的stream处理。对这些知识的理解除了需要掌握一点有关广告系统的业务常识以外，也需要对Kafka Streams的理论基础与实际使用有所了解。

### 小Q，来自腾讯的AI实践

今天的最后一场演讲，我选择了来自腾讯的“小Q机器人的诞生之路“，也是想借此机会了解和学习一下国内IT领域在AI方面的应用现状。这是今天下午听到的较为轻松的一个话题，也许是因为困意渐消的缘故吧。小Q机器人是腾讯研发的一款AI产品，包括软件和硬件部分，自2010年问世以来，已经历了两代更迭，目前二代机器人已于2017年上市。

小Q产品的发展，可以追溯到当年QQ空间的聊天机器人。其AI系统是针对基于SQL的全文检索引擎Sphinx改造而成的。依托于腾讯的海量聊天数据，10亿级中文文档，其规模超越了另一款国人基于Sphinx打造的中文检索方案coreseek。从系统架构可以看出，小Q的后台是典型的全文检索系统，包含了数据准备，索引，查询，检索等几大模块。其中，为了提高检索的准确性，基于海量数据的数据清洗和对检索结果的Ranking尤其重要，这也是演讲中着墨较多的部分。对于Ranking的处理，腾讯采用了多种方法来提高质量，比如：采用基于条件概率的词共现算法，通过对海量聊天记录的统计，得到聊天对话中语义上共现的规律；并结合POS-IDF词向量权重模型，从语义角度进一步对条件概率公式作出修正；再结合深度学习CNN模型的改良模型PairCNN，使Ranking结果较之baseline获得了近70%的提升。

小Q的AI后台是通过腾讯云的“小微”云服务与硬件终端小Q机器人进行联通的。小微整合了腾讯内部来自不同部门的多项AI能力，包括：来自微信，AILabs等团队的语音识别，语义，及自然语言处理技术；来自优图实验室，天天P图团队的文字转语音及人脸识别技术；来自腾讯物联，QQ音乐，腾讯视频等团队的内容整合与语料提供等。可以看到，貌似简单的一个小小机器人，其后端调动的却是腾讯内部的全部AI技术资源。这其中也包括在不同环节不断的逐级优化，比如：基于fastText进行领域预分类，以提高响应速率；利用rwnd+调整策略改进语义相似度算法，以提高正确率；采用GBDT+Boosting进行实体消岐，以提高准确率；利用Query Correction进行口语化处理，以提高容错能力。从而令意图的正确率达到业界同类产品中的较高水准。

小Q的硬件部分也颇值得说道。由于作为软件研发团队天然缺乏硬件基因，加上硬件项目从研发到量产的周期长、风险大，腾讯采用的策略是：在成熟的硬件产品基础之上，通过原型机快速打磨并完善软件，然后再进行软硬结合的整体优化。在优化方面，演讲中着重提到了响应速度和唤醒成功率。对于响应速度，优化策略包括：服务就近部署，数据缓存及预加载，流式处理等措施。比如：将语音识别和语义分析改为流式计算，检测到静音的同时就能拿到语义结果；播放TTS时，预加载音乐内容，TTS播放结束音乐即可立即播放。对于唤醒成功率，则与硬件设备关系更为紧密，涉及硬件调优。比如：开启硬件回声消除（AEC）功能，调整放大增益参数以确保输入信号的完整性等。最后，演讲也谈及了硬件项目中普遍存在的各种风险，比如：变更设计的成本很高，供应链波动导致产品风险，以及由良品率与毁灭性bug带来的产品质量风险，这些都是纯软件研发人员所未曾接触过的，非常受用。

（待续）